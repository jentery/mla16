# Computer Vision as a Public Act: On Digital Humanities and Algocracy 

Computer vision is generally associated with the programmatic description and reconstruction of the physical world in digital form (Szeliski 2010: 3-10). It helps people construct and express visual patterns in data, such as patterns in image, video, and text repositories. The processes involved in this recognition are incredibly tedious, hence tendencies to automate them with algorithms. They are also increasingly common in everyday life, expanding the role of algorithms in the reproduction of culture. From the perspective of economic sociology, A. Aneesh links such expansion to "a new kind of power" and governance, which he refers to as "*algocracy*—rule of the algorithm, or rule of the code" (Aneesh 2006: 5). Here, the programmatic treatment of the physical world in digital form is so significantly embedded in infrastructures that algorithms tacitly shape behaviors and prosaically assert authority in tandem with existing bureaucracies. Routine decisions are delegated (knowingly or not) to computational procedures that—echoing the work of Alexander Galloway (2001), Wendy Chun (2011), and many others in media studies—run in the background as protocols or default settings. For the purposes of this MLA panel, I am specifically interested in how humanities researchers may not only interpret computer vision as a public act but also intervene in it through a sort of “critical technical practice” (Agre 1997: 155) advocated by digital humanities scholars such as Tara McPherson (2012) and Alan Liu (2012). In the case of computer vision, such critical technical practice may begin by relinquishing any assumptions that computer vision can be fully understood as, or simply reduced to, "source code." Then, without eagerly adopting new technologies, it might proceed to ask whether (if ever) and how computer vision should be used for humanities research. In short, *digital humanities must more rigorously engage the question of treating bodies as big data*, especially as the oppressive and algocratic effects of computer vision become all the more palpable in everyday life.  

According to various accounts, computer vision research began as early as 1966, during the "Summer Vision Project," when Marvin Minsky, Seymour Papert, Gerald Jay Sussman, and others in the Artificial Intelligence Group (AIG) at the Massachusetts Institute of Technology (MIT) investigated how to use figure-ground analysis to automatically divide pictures into regions based on surface and shape properties. This region description would act as the basis for object identification, where items in pictures were recognized and named by machines with controlled vocabularies of known objects (Pappert 1966; Crevier 1993; Boden 2006). Cameras were attached to computers in order to achieve this automated description and identification, with an express desire to eventually construct a pattern analysis system that would combine "maximal flexibility" with "heuristic rules" (Papert 1966: 6). 

Although computer vision has developed significantly since the 1960s and ’70s, the AIG’s Summer Vision Project marks a notable transition in media history, a moment when researchers began integrating image processing into the development of artificial intelligence, including the training of computers to read, detect, and describe aspects of pictures and visual environments (Szeliski 2010: 7-10). During the project, AIG researchers also started asking computer vision questions that, if only tacitly, have persisted: How does computer vision differ from human vision? To what degree should computer vision be modeled on human phenomenology, and to what effects? Can computer or human vision even be modeled? That is, can either even be generalized? Where and when do issues of processing and memory matter most for recognition and description? And how should computer vision handle ambiguity? (Minsky 1974). These questions are at once technical and ideological, as are many responses to them, meaning computer vision (then or now) should be be extracted from the contexts of its conceptual and material development.   

Today, computer vision has moved, at least in part, from laboratories into consumer technologies. One popular application is answering the question, "Is this you?" or "Is this them?" iPhoto, Facebook, and Kinect users are intimately familiar with this application, where face detection algorithms analyze patterns to calculate a core or integral image within an image, assess differences across a spectrum of light, and view images across scales. In the open source community, many practitioners combine the Open Source Computer Vision (OpenCV) library with the Python, C++, and Java programming languages to perform this "detection" work. These scripts rely on frameworks to train classifiers to detect "objects"—which, in the language of vision science, include faces, bodies, and body parts—in images based on cascades of features. To see faces while algorithmically scanning images, OpenCV uses the widely popular Viola-Jones object detection framework that relies on "Haar-like" image features for cascading (Viola and Jones 2004). Similar to iPhoto and other image management applications, OpenCV can be used to "identify"—often with errors and omissions—the same face across a distribution, even when multiple faces appear in the same image. Here, I surround terms such as detection and identify with quotation marks because, while this is the langauge used in vision science, computer vision actively helps produce the patterns it ostensibly discovers in bodies and environments. That is, in order to recognize patterns, it must first perform a translation or remediation. 

Even more important, to write computer vision scripts, *programmers do not even need to know—or, to be clear, even understand—the particulars of Haar cascades or Viola-Jones. Their scripts simply call and deploy "trusted" cascades* (e.g., "Frontal Face," "Human Body," "Wall Clock," and "Mouth") stored in XML files readily available across the web. Once a computer vision script detects objects in a given cascade, it can extract them from their source files and archive them. 

Computer vision techniques may also merge or compare extracted objects with existing objects. Comparisons allow people to confirm or complicate known relationships between objects. For instance, when comparing faces, multiple photos of the same person can train algorithms to recognize an "eigenface" (or "unique face") generated from the principle components of those photos. *Although eigenfaces do not actually exist in any lived, social reality, they are fundamental to the process of face recognition, and datasets with "training face" images for 100+ people per repository are now common online.* One of the most popular sets is the Public Figures Face Database (Pubfig), a "real-world face dataset" consisting of 58,797 internet images of 200 "non-cooperative subjects" that were "taken in completely uncontrolled situations" (Columbia University 2010). While this and other face datasets suggest that training faces are central to big data initiatives anchored in computer vision, humanities practitioners have not thoroughly considered the social and cultural implications of treating bodies as big data for vision science. Indeed, much more humanities research is needed in this area, especially as it relates to policing and racial profiling.  


